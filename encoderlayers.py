# -*- coding: utf-8 -*-
"""EncoderLayers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1My-t0oOcqPO7ySdIQEkeEc2yA0NfNBu9
"""

-----------------------------ENCODING LAYERS EncoderLayer(
  (temporal_self_attention): TemporalSelfAttention(
    (meta_learner): MetaLearner(
      (linear1): Linear(in_features=16, out_features=16, bias=True)
      (relu): ReLU()
      (linear2): Linear(in_features=16, out_features=768, bias=True)
    )
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (linear): Linear(in_features=16, out_features=16, bias=False)
  )
  (spatial_self_attention): SpatialSelfAttention(
    (meta_learners): ModuleList(
      (0-2): 3 x MetaLearner(
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (relu): ReLU()
        (linear2): Linear(in_features=16, out_features=768, bias=True)
      )
    )
    (linear): Linear(in_features=48, out_features=16, bias=False)
    (dropout): Dropout(p=0.3, inplace=False)
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
  )
  (feed_forward): FeedForward(
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (relu): ReLU()
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
  )
) ----------------------------------
........................................
EcoderLayer inputs torch.Size([2, 4, 80, 16]) 10240
EcoderLayer c_inputs torch.Size([2, 4, 10, 16]) 1280
EcoderLayer transition_matrices torch.Size([3, 80, 80]) 19200
.......................................

---- encoder ----
---TemporalSelfAttention---
inputs torch.Size([2, 4, 80, 16]) 10240
c_inputs torch.Size([2, 4, 10, 16]) 10240
MetaLearner torch.Size([2, 4, 10, 16]) 1280

outMetaLearnerLast torch.Size([3, 2, 4, 10, 4, 4, 16]) 61440
W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

encoder Mwalemi temporal attention torch.Size([2, 4, 80, 16])  #TAO

-- encoder ---
----SpatialSelfAttention---
inputs torch.Size([2, 4, 80, 16]) 10240
c_inputs torch.Size([2, 4, 10, 16]) 1280
MetaLearner torch.Size([2, 4, 10, 16]) 1280

outMetaLearnerLast torch.Size([3, 2, 4, 10, 4, 4, 16]) 61440
W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

MetaLearner torch.Size([2, 4, 10, 16]) 1280

outMetaLearnerLast torch.Size([3, 2, 4, 10, 4, 4, 16]) 61440
W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

MetaLearner torch.Size([2, 4, 10, 16]) 1280

outMetaLearnerLast torch.Size([3, 2, 4, 10, 4, 4, 16]) 61440
W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

W  torch.Size([2, 4, 10, 4, 4, 16]) 20480
inputs torch.Size([2, 4, 80, 16]) 10240

encoder Spatial attention  torch.Size([2, 4, 80, 16]) 10240 ##SAO

out TemporalEncoderDecoderAttention torch.Size([2, 4, 80, 16]) 10240 #Hybrid

out EncoderLayer torch.Size([2, 4, 80, 16]) 10240  #ouput for Layer 1