# -*- coding: utf-8 -*-
"""DecoderLayers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CFJl8ZwXbnGrGvgpaU-l0uSCy6i6hzDG
"""

-----------------------------DECODER LAYER DecoderLayer(
  (temporal_self_attention): TemporalSelfAttention(
    (meta_learner): MetaLearner(
      (linear1): Linear(in_features=16, out_features=16, bias=True)
      (relu): ReLU()
      (linear2): Linear(in_features=16, out_features=768, bias=True)
    )
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (linear): Linear(in_features=16, out_features=16, bias=False)
  )
  (spatial_self_attention): SpatialSelfAttention(
    (meta_learners): ModuleList(
      (0-2): 3 x MetaLearner(
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (relu): ReLU()
        (linear2): Linear(in_features=16, out_features=768, bias=True)
      )
    )
    (linear): Linear(in_features=48, out_features=16, bias=False)
    (dropout): Dropout(p=0.3, inplace=False)
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
  )
  (temporal_encoder_decoder_attention): TemporalEncoderDecoderAttention(
    (meta_learner): MetaLearner(
      (linear1): Linear(in_features=16, out_features=16, bias=True)
      (relu): ReLU()
      (linear2): Linear(in_features=16, out_features=256, bias=True)
    )
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (linear): Linear(in_features=16, out_features=16, bias=False)
  )
  (feed_forward): FeedForward(
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (relu): ReLU()
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
  )
) ----------------------------------
DecoderLayer inputs torch.Size([2, 1, 160, 16]) 5120
DecoderLayer inputs_enc_K torch.Size([2, 4, 10, 4, 4, 8]) 10240
DecoderLayer inputs_enc_V torch.Size([2, 4, 10, 4, 4, 8]) 10240
DecoderLayer inputs_c_targets torch.Size([2, 1, 10, 16]) 320

---- Decoder ----
TemporalSelfAttention
inputs torch.Size([2, 1, 160, 16]) 5120
c_inputs torch.Size([2, 1, 10, 16]) 5120
MetaLearner torch.Size([2, 1, 10, 16]) 320

outMetaLearnerLast torch.Size([3, 2, 1, 10, 4, 4, 16]) 15360
W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 1, 160, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 1, 160, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 1, 160, 16]) 5120

Decoder Mwalemi temporal attention torch.Size([2, 2, 80, 16]) #DTAO

-- Decoder ---
SpatialSelfAttention
inputs torch.Size([2, 2, 80, 16]) 5120
c_inputs torch.Size([2, 1, 10, 16]) 320
MetaLearner torch.Size([2, 1, 10, 16]) 320

outMetaLearnerLast torch.Size([3, 2, 1, 10, 4, 4, 16]) 15360
W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

MetaLearner torch.Size([2, 1, 10, 16]) 320

outMetaLearnerLast torch.Size([3, 2, 1, 10, 4, 4, 16]) 15360
W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

MetaLearner torch.Size([2, 1, 10, 16]) 320

outMetaLearnerLast torch.Size([3, 2, 1, 10, 4, 4, 16]) 15360
W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

Decoder Spatial attention  torch.Size([2, 2, 80, 16]) 5120  #DSAO

inputs torch.Size([2, 2, 80, 16]) 5120
E&D enc_K torch.Size([2, 4, 10, 4, 4, 8]) 10240
E&D enc_V torch.Size([2, 4, 10, 4, 4, 8]) 10240
E&D c_targets torch.Size([2, 1, 10, 16]) 320
MetaLearner torch.Size([2, 1, 10, 16]) 320

outMetaLearnerLast torch.Size([1, 2, 1, 10, 4, 4, 16]) 5120
W  torch.Size([2, 1, 10, 4, 4, 16]) 5120
inputs torch.Size([2, 2, 80, 16]) 5120

Burundi and RWANDA torch.Size([2, 2, 160, 16]) 10240
out TemporalEncoderDecoderAttention torch.Size([2, 2, 160, 16]) 10240  ##DSTAO

This is our target torch.Size([2, 2, 160, 16]) 10240